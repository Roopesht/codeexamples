{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "3P7uPaml_qW3"
            },
            "source": [
                "## Demonstration QLORA"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Setup "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run this only if needed (if you get error in the next cell)\n",
                "!pip install -U bitsandbytes transformers accelerate peft datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# verify if GPU is there\n",
                "import torch\n",
                "torch.cuda.is_available()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check the GPU\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "AO6XMAVhCBHm"
            },
            "outputs": [],
            "source": [
                "# Load the model (QLORA version)\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "import torch\n",
                "\n",
                "model_name = \"distilgpt2\"\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,   # ðŸ‘ˆ QLoRA\n",
                "    device_map=\"auto\"\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "sF8eXUuuCHRn"
            },
            "outputs": [],
            "source": [
                "prompt = \"Answer politely: How do I reset my password?\"\n",
                "\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "out = model.generate(\n",
                "    **inputs,\n",
                "    max_new_tokens=60,\n",
                "    do_sample=True\n",
                ")\n",
                "\n",
                "generated = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
                "\n",
                "print(\"BEFORE TRAINING (QLoRA):\\n\")\n",
                "print(tokenizer.decode(generated, skip_special_tokens=True))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "NaZ1DcQQCXqW"
            },
            "source": [
                "### Training\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "GRoKpmDHCRHR"
            },
            "outputs": [],
            "source": [
                "train_texts = [\n",
                "    \"You are a polite customer support agent.\\nQuestion: How do I reset my password?\\nAnswer: Please click 'Forgot Password' and follow the email instructions.\",\n",
                "    \"You are a polite customer support agent.\\nQuestion: How can I contact support?\\nAnswer: You can email us at support@company.com.\",\n",
                "    \"You are a polite customer support agent.\\nQuestion: Where is my order?\\nAnswer: I will gladly check your order status for you.\",\n",
                "    \"You are a polite customer support agent.\\nQuestion: Can I return a product?\\nAnswer: Yes, returns are accepted within 30 days.\",\n",
                "    \"You are a polite customer support agent.\\nQuestion: What are your hours?\\nAnswer: We are open Monday to Friday, 9 AM to 5 PM.\"\n",
                "]\n",
                "\n",
                "from datasets import Dataset\n",
                "\n",
                "dataset = Dataset.from_dict({\"text\": train_texts})\n",
                "\n",
                "def tokenize(batch):\n",
                "    tokens = tokenizer(\n",
                "        batch[\"text\"],\n",
                "        truncation=True,\n",
                "        padding=\"max_length\",\n",
                "        max_length=128\n",
                "    )\n",
                "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # ðŸ”‘ REQUIRED\n",
                "    return tokens\n",
                "\n",
                "dataset = dataset.map(tokenize, batched=True)\n",
                "dataset.set_format(\"torch\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "xzcwH-l1_ssX"
            },
            "outputs": [],
            "source": [
                "# QLoRA = Quantized model + LoRA\n",
                "from peft import LoraConfig, get_peft_model\n",
                "\n",
                "lora_config = LoraConfig(\n",
                "    r=8,\n",
                "    lora_alpha=16,\n",
                "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 style\n",
                "    lora_dropout=0.05,\n",
                "    task_type=\"CAUSAL_LM\"\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "o9HR3lAuDUXQ"
            },
            "outputs": [],
            "source": [
                "from transformers import TrainingArguments, Trainer\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./demo_qlora\",\n",
                "    per_device_train_batch_size=1,\n",
                "    max_steps=10,\n",
                "    logging_steps=1,\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=dataset\n",
                ")\n",
                "\n",
                "trainer.train()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "n4dz5x6pDlHa"
            },
            "source": [
                "### Test again"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Rurjty1EDWP4"
            },
            "outputs": [],
            "source": [
                "out = model.generate(\n",
                "    **inputs,\n",
                "    max_new_tokens=60,\n",
                "    do_sample=True\n",
                ")\n",
                "\n",
                "generated = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
                "\n",
                "print(\"AFTER TRAINING (QLoRA):\\n\")\n",
                "print(tokenizer.decode(generated, skip_special_tokens=True))\n"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "authorship_tag": "ABX9TyNQPyBwllOVMHH2GK8gX6sz",
            "collapsed_sections": [
                "Zf7junAgE6Ws",
                "NaZ1DcQQCXqW"
            ],
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
