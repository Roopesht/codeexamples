{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Zf7junAgE6Ws"
            },
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "dxZUVBrFE53_"
            },
            "outputs": [],
            "source": [
                "\n",
                "!pip install -q transformers accelerate peft bitsandbytes\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# verify if GPU is there\n",
                "import torch\n",
                "torch.cuda.is_available()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check the GPU\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ptQnt6inI9iZ"
            },
            "source": [
                "## Demonstration LORA"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9UD8LIN6WUrA"
            },
            "source": [
                "### Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "id": "EHSVnQwQJDZJ"
            },
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "import torch\n",
                "\n",
                "model_name = \"microsoft/phi-2\"\n",
                "#model_name = \"distilgpt2\"\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    dtype=torch.float16,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "F1hYhtHobBOC"
            },
            "outputs": [],
            "source": [
                "system_message = \"You are a helpful Assistant\"\n",
                "q1=\"how can I change my password?\"\n",
                "q2 = \"I keep procrastinating and lack discipline. What would Chanakya advise?\"\n",
                "q3=\"what to do if I forgoet my password?\"\n",
                "q4=\"Please help me to reset my password\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "nhSU6nWWWTEo"
            },
            "outputs": [],
            "source": [
                "\n",
                "def ask_genai_course_advisor(question, model, tokenizer, max_new_tokens=60):\n",
                "    \"\"\"\n",
                "    Ask the fine-tuned model a question.\n",
                "    \"\"\"\n",
                "\n",
                "    prompt = f\"\"\"{system_message}.\n",
                "Question: {question}\n",
                "Answer:\"\"\"\n",
                "\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        out = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_new_tokens,\n",
                "            repetition_penalty=1.3,\n",
                "            no_repeat_ngram_size=3,\n",
                "            temperature=0.0,\n",
                "            do_sample=False\n",
                "        )\n",
                "\n",
                "    generated = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
                "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LMaqkgdRI_kU"
            },
            "source": [
                "### Before"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "RsKzFTeDJBZh"
            },
            "outputs": [],
            "source": [
                "# inputs are already calculated in \"BEFORE block\"\n",
                "# Model is in memory\n",
                "\n",
                "answer = ask_genai_course_advisor(\n",
                "    q2,\n",
                "    model,\n",
                "    tokenizer\n",
                ")\n",
                "\n",
                "print(answer)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "YqC8AYwOLcw2"
            },
            "source": [
                "### Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "hx-jwPA4Jyrp"
            },
            "outputs": [],
            "source": [
                "from datasets import Dataset\n",
                "\n",
                "# Training data split into question + answer (explicit structure)\n",
                "train_data = [\n",
                "    {\n",
                "        \"question\": \"I feel distracted all the time. How can I improve my focus?\",\n",
                "        \"answer\": \"Chanakya teaches that the mind follows discipline. Reduce unnecessary desires, set one clear goal, and eliminate distractions without mercy.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"How did Chanakya view focus and self-control?\",\n",
                "        \"answer\": \"According to Chanakya, focus is born from self-control. A person who cannot govern their senses cannot govern their destiny.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is Chanakyaâ€™s advice for mental clarity?\",\n",
                "        \"answer\": \"Chanakya advised waking early, maintaining routine, and avoiding indulgence. A structured life creates a focused mind.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"How can I train my mind to stay focused?\",\n",
                "        \"answer\": \"Chanakya believed the mind must be trained like a warrior. Daily discipline, limited comforts, and repeated practice sharpen focus.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"Why do I lose focus easily according to Chanakya?\",\n",
                "        \"answer\": \"Chanakya taught that lack of purpose weakens focus. When the goal is unclear, the mind wanders without direction.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What habits destroy focus?\",\n",
                "        \"answer\": \"Chanakya warned against excess sleep, uncontrolled speech, and constant pleasure-seeking, as these slowly destroy concentration.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"How does one build strong concentration?\",\n",
                "        \"answer\": \"Chanakya advised doing one task at a time with full attention. Divided effort leads to divided results.\"\n",
                "    }\n",
                "]\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "dataset = Dataset.from_list(train_data)\n",
                "\n",
                "def tokenize(batch):\n",
                "    input_ids = []\n",
                "    labels = []\n",
                "    attention_masks = []\n",
                "\n",
                "    for q, a in zip(batch[\"question\"], batch[\"answer\"]):\n",
                "        prompt = f\"{system_message}.\\nQuestion: {q}\\nAnswer:\"\n",
                "        answer = f\" {a}\"\n",
                "\n",
                "        # Tokenize prompt + answer together\n",
                "        full = tokenizer(\n",
                "            prompt + answer,\n",
                "            truncation=True,\n",
                "            padding=\"max_length\",\n",
                "            max_length=128\n",
                "        )\n",
                "\n",
                "        # Tokenize prompt alone (to know where answer starts)\n",
                "        prompt_tokens = tokenizer(prompt, truncation=True, max_length=128)\n",
                "\n",
                "        label = [-100] * len(full[\"input_ids\"])\n",
                "\n",
                "        answer_start = len(prompt_tokens[\"input_ids\"])\n",
                "        label[answer_start:] = full[\"input_ids\"][answer_start:]\n",
                "\n",
                "        input_ids.append(full[\"input_ids\"])\n",
                "        labels.append(label)\n",
                "        attention_masks.append(full[\"attention_mask\"])\n",
                "\n",
                "    return {\n",
                "        \"input_ids\": input_ids,\n",
                "        \"labels\": labels,\n",
                "        \"attention_mask\": attention_masks\n",
                "    }\n",
                "\n",
                "dataset = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
                "dataset.set_format(\"torch\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "epN_5sWTJIWZ"
            },
            "outputs": [],
            "source": [
                "# LORA\n",
                "from peft import LoraConfig, get_peft_model\n",
                "\n",
                "if model_name == \"distilgpt2\":\n",
                "  lora_config = LoraConfig(\n",
                "      r=8,\n",
                "      lora_alpha=16,\n",
                "      target_modules=[\"c_attn\", \"c_proj\"],\n",
                "      lora_dropout=0.05,\n",
                "      task_type=\"CAUSAL_LM\"\n",
                "  )\n",
                "else:\n",
                "  lora_config = LoraConfig(\n",
                "      r=8,\n",
                "      lora_alpha=16,\n",
                "      target_modules=[\"q_proj\", \"v_proj\"],\n",
                "      lora_dropout=0.05,\n",
                "      task_type=\"CAUSAL_LM\"\n",
                "  )\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "\n",
                "model.print_trainable_parameters()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "FD9phwoPJpqj"
            },
            "outputs": [],
            "source": [
                "from transformers import TrainingArguments, Trainer\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./demo\",\n",
                "    per_device_train_batch_size=1,\n",
                "    num_train_epochs=1,\n",
                "    max_steps=80,\n",
                "    logging_steps=1,\n",
                "    fp16=True,\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=dataset\n",
                ")\n",
                "\n",
                "trainer.train()\n",
                "\n",
                "model.eval()\n",
                "model.print_trainable_parameters()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "_9eSgL1ehjqp"
            },
            "outputs": [],
            "source": [
                "# Save and reopen\n",
                "\"\"\"\n",
                "model.save_pretrained(\"./demo_updated\")\n",
                "tokenizer.save_pretrained(\"./demo_updated\")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    device_map=\"auto\",\n",
                "    torch_dtype=torch.float16\n",
                ")\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.load_adapter(\"./demo_updated\")\n",
                "\"\"\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# alternate way of directly updating the model in memory\n",
                "model = model.merge_and_unload()\n",
                "model.eval()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9AOLR41MLqYh"
            },
            "source": [
                "### After"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ANWrXqNCXb3M"
            },
            "outputs": [],
            "source": [
                "# inputs are already calculated in \"BEFORE block\"\n",
                "# Model is in memory\n",
                "#question= \"Is there any interview support?\"\n",
                "answer = ask_genai_course_advisor(\n",
                "    q2,\n",
                "    model,\n",
                "    tokenizer\n",
                ")\n",
                "\n",
                "print(answer)\n"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "authorship_tag": "ABX9TyNQPyBwllOVMHH2GK8gX6sz",
            "collapsed_sections": [
                "Zf7junAgE6Ws",
                "NaZ1DcQQCXqW"
            ],
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
