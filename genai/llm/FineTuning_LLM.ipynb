{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "collapsed_sections": [
                "Zf7junAgE6Ws",
                "NaZ1DcQQCXqW"
            ],
            "gpuType": "T4",
            "authorship_tag": "ABX9TyNQPyBwllOVMHH2GK8gX6sz"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Setup"
            ],
            "metadata": {
                "id": "Zf7junAgE6Ws"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install -q transformers accelerate peft bitsandbytes\n",
                "\n"
            ],
            "metadata": {
                "id": "dxZUVBrFE53_"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model"
            ],
            "metadata": {
                "id": "_6ewXoLIFOtx"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "model_name = \"microsoft/phi-2\"\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\"\n",
                ")"
            ],
            "metadata": {
                "id": "VdEff5raFAPf"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [],
            "metadata": {
                "id": "mym0nMC6E97X"
            }
        },
        {
            "cell_type": "code",
            "source": [],
            "metadata": {
                "id": "THcAkz6qE-Dl"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Demonstration LORA"
            ],
            "metadata": {
                "id": "ptQnt6inI9iZ"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Setup"
            ],
            "metadata": {
                "id": "9UD8LIN6WUrA"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "import torch\n",
                "\n",
                "#model_name = \"microsoft/phi-2\"\n",
                "model_name = \"distilgpt2\"\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    dtype=torch.float16,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n"
            ],
            "metadata": {
                "id": "EHSVnQwQJDZJ",
                "collapsed": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "system_message = \"You are a helpful Assistant\"\n",
                "q1=\"how can I change my password?\"\n",
                "q2 = \"I keep procrastinating and lack discipline. What would Chanakya advise?\"\n",
                "q3=\"what to do if I forgoet my password?\"\n",
                "q4=\"Please help me to reset my password\""
            ],
            "metadata": {
                "id": "F1hYhtHobBOC"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "\n",
                "def ask_genai_course_advisor(question, model, tokenizer, max_new_tokens=60):\n",
                "    \"\"\"\n",
                "    Ask the fine-tuned model a question.\n",
                "    \"\"\"\n",
                "\n",
                "    prompt = f\"\"\"{system_message}.\n",
                "Question: {question}\n",
                "Answer:\"\"\"\n",
                "\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        out = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_new_tokens,\n",
                "            repetition_penalty=1.3,\n",
                "            no_repeat_ngram_size=3,\n",
                "            temperature=0.0,\n",
                "            do_sample=False\n",
                "        )\n",
                "\n",
                "    generated = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
                "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n"
            ],
            "metadata": {
                "id": "nhSU6nWWWTEo"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Before"
            ],
            "metadata": {
                "id": "LMaqkgdRI_kU"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# inputs are already calculated in \"BEFORE block\"\n",
                "# Model is in memory\n",
                "\n",
                "answer = ask_genai_course_advisor(\n",
                "    q2,\n",
                "    model,\n",
                "    tokenizer\n",
                ")\n",
                "\n",
                "print(answer)\n"
            ],
            "metadata": {
                "id": "RsKzFTeDJBZh"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Training"
            ],
            "metadata": {
                "id": "YqC8AYwOLcw2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from datasets import Dataset\n",
                "\n",
                "# Training data split into question + answer (explicit structure)\n",
                "train_data = [\n",
                "    {\n",
                "        \"question\": \"I feel distracted all the time. How can I improve my focus?\",\n",
                "        \"answer\": \"Chanakya teaches that the mind follows discipline. Reduce unnecessary desires, set one clear goal, and eliminate distractions without mercy.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"How did Chanakya view focus and self-control?\",\n",
                "        \"answer\": \"According to Chanakya, focus is born from self-control. A person who cannot govern their senses cannot govern their destiny.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is Chanakyaâ€™s advice for mental clarity?\",\n",
                "        \"answer\": \"Chanakya advised waking early, maintaining routine, and avoiding indulgence. A structured life creates a focused mind.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"How can I train my mind to stay focused?\",\n",
                "        \"answer\": \"Chanakya believed the mind must be trained like a warrior. Daily discipline, limited comforts, and repeated practice sharpen focus.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"Why do I lose focus easily according to Chanakya?\",\n",
                "        \"answer\": \"Chanakya taught that lack of purpose weakens focus. When the goal is unclear, the mind wanders without direction.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What habits destroy focus?\",\n",
                "        \"answer\": \"Chanakya warned against excess sleep, uncontrolled speech, and constant pleasure-seeking, as these slowly destroy concentration.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"How does one build strong concentration?\",\n",
                "        \"answer\": \"Chanakya advised doing one task at a time with full attention. Divided effort leads to divided results.\"\n",
                "    }\n",
                "]\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "dataset = Dataset.from_list(train_data)\n",
                "\n",
                "def tokenize(batch):\n",
                "    input_ids = []\n",
                "    labels = []\n",
                "    attention_masks = []\n",
                "\n",
                "    for q, a in zip(batch[\"question\"], batch[\"answer\"]):\n",
                "        prompt = f\"{system_message}.\\nQuestion: {q}\\nAnswer:\"\n",
                "        answer = f\" {a}\"\n",
                "\n",
                "        # Tokenize prompt + answer together\n",
                "        full = tokenizer(\n",
                "            prompt + answer,\n",
                "            truncation=True,\n",
                "            padding=\"max_length\",\n",
                "            max_length=128\n",
                "        )\n",
                "\n",
                "        # Tokenize prompt alone (to know where answer starts)\n",
                "        prompt_tokens = tokenizer(prompt, truncation=True, max_length=128)\n",
                "\n",
                "        label = [-100] * len(full[\"input_ids\"])\n",
                "\n",
                "        answer_start = len(prompt_tokens[\"input_ids\"])\n",
                "        label[answer_start:] = full[\"input_ids\"][answer_start:]\n",
                "\n",
                "        input_ids.append(full[\"input_ids\"])\n",
                "        labels.append(label)\n",
                "        attention_masks.append(full[\"attention_mask\"])\n",
                "\n",
                "    return {\n",
                "        \"input_ids\": input_ids,\n",
                "        \"labels\": labels,\n",
                "        \"attention_mask\": attention_masks\n",
                "    }\n",
                "\n",
                "dataset = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
                "dataset.set_format(\"torch\")\n"
            ],
            "metadata": {
                "id": "hx-jwPA4Jyrp"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# LORA\n",
                "from peft import LoraConfig, get_peft_model\n",
                "\n",
                "if model_name == \"distilgpt2\":\n",
                "  lora_config = LoraConfig(\n",
                "      r=8,\n",
                "      lora_alpha=16,\n",
                "      target_modules=[\"c_attn\", \"c_proj\"],\n",
                "      lora_dropout=0.05,\n",
                "      task_type=\"CAUSAL_LM\"\n",
                "  )\n",
                "else:\n",
                "  lora_config = LoraConfig(\n",
                "      r=8,\n",
                "      lora_alpha=16,\n",
                "      target_modules=[\"q_proj\", \"v_proj\"],\n",
                "      lora_dropout=0.05,\n",
                "      task_type=\"CAUSAL_LM\"\n",
                "  )\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "\n",
                "model.print_trainable_parameters()\n",
                "\n"
            ],
            "metadata": {
                "id": "epN_5sWTJIWZ"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "from transformers import TrainingArguments, Trainer\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./demo\",\n",
                "    per_device_train_batch_size=1,\n",
                "    num_train_epochs=1,\n",
                "    max_steps=80,\n",
                "    logging_steps=1,\n",
                "    fp16=True,\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=dataset\n",
                ")\n",
                "\n",
                "trainer.train()\n",
                "\n",
                "model.eval()\n",
                "model.print_trainable_parameters()\n",
                "\n"
            ],
            "metadata": {
                "id": "FD9phwoPJpqj"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Save\n",
                "#model.save_pretrained(\"./demo_updated\")\n",
                "#tokenizer.save_pretrained(\"./demo_updated\")"
            ],
            "metadata": {
                "id": "_9eSgL1ehjqp"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### After"
            ],
            "metadata": {
                "id": "9AOLR41MLqYh"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# inputs are already calculated in \"BEFORE block\"\n",
                "# Model is in memory\n",
                "#question= \"Is there any interview support?\"\n",
                "answer = ask_genai_course_advisor(\n",
                "    q2,\n",
                "    model,\n",
                "    tokenizer\n",
                ")\n",
                "\n",
                "print(answer)\n"
            ],
            "metadata": {
                "id": "ANWrXqNCXb3M"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Demonstration QLORA"
            ],
            "metadata": {
                "id": "3P7uPaml_qW3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Load the model (QLORA version)\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "import torch\n",
                "\n",
                "model_name = \"distilgpt2\"\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,   # ðŸ‘ˆ QLoRA\n",
                "    device_map=\"auto\"\n",
                ")\n"
            ],
            "metadata": {
                "id": "AO6XMAVhCBHm"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "prompt = \"Answer politely: How do I reset my password?\"\n",
                "\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "out = model.generate(\n",
                "    **inputs,\n",
                "    max_new_tokens=60,\n",
                "    do_sample=True\n",
                ")\n",
                "\n",
                "generated = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
                "\n",
                "print(\"BEFORE TRAINING (QLoRA):\\n\")\n",
                "print(tokenizer.decode(generated, skip_special_tokens=True))\n"
            ],
            "metadata": {
                "id": "sF8eXUuuCHRn"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Training\n"
            ],
            "metadata": {
                "id": "NaZ1DcQQCXqW"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "train_texts = [\n",
                "    \"You are a polite customer support agent.\\nQuestion: How do I reset my password?\\nAnswer: Please click 'Forgot Password' and follow the email instructions.\",\n",
                "    \"You are a polite customer support agent.\\nQuestion: How can I contact support?\\nAnswer: You can email us at support@company.com.\",\n",
                "    \"You are a polite customer support agent.\\nQuestion: Where is my order?\\nAnswer: I will gladly check your order status for you.\",\n",
                "    \"You are a polite customer support agent.\\nQuestion: Can I return a product?\\nAnswer: Yes, returns are accepted within 30 days.\",\n",
                "    \"You are a polite customer support agent.\\nQuestion: What are your hours?\\nAnswer: We are open Monday to Friday, 9 AM to 5 PM.\"\n",
                "]\n",
                "\n",
                "from datasets import Dataset\n",
                "\n",
                "dataset = Dataset.from_dict({\"text\": train_texts})\n",
                "\n",
                "def tokenize(batch):\n",
                "    tokens = tokenizer(\n",
                "        batch[\"text\"],\n",
                "        truncation=True,\n",
                "        padding=\"max_length\",\n",
                "        max_length=128\n",
                "    )\n",
                "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # ðŸ”‘ REQUIRED\n",
                "    return tokens\n",
                "\n",
                "dataset = dataset.map(tokenize, batched=True)\n",
                "dataset.set_format(\"torch\")\n",
                "\n"
            ],
            "metadata": {
                "id": "GRoKpmDHCRHR"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# QLoRA = Quantized model + LoRA\n",
                "from peft import LoraConfig, get_peft_model\n",
                "\n",
                "lora_config = LoraConfig(\n",
                "    r=8,\n",
                "    lora_alpha=16,\n",
                "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 style\n",
                "    lora_dropout=0.05,\n",
                "    task_type=\"CAUSAL_LM\"\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n"
            ],
            "metadata": {
                "id": "xzcwH-l1_ssX"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "from transformers import TrainingArguments, Trainer\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./demo_qlora\",\n",
                "    per_device_train_batch_size=1,\n",
                "    max_steps=10,\n",
                "    logging_steps=1,\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=dataset\n",
                ")\n",
                "\n",
                "trainer.train()\n"
            ],
            "metadata": {
                "id": "o9HR3lAuDUXQ"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Test again"
            ],
            "metadata": {
                "id": "n4dz5x6pDlHa"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "out = model.generate(\n",
                "    **inputs,\n",
                "    max_new_tokens=60,\n",
                "    do_sample=True\n",
                ")\n",
                "\n",
                "generated = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
                "\n",
                "print(\"AFTER TRAINING (QLoRA):\\n\")\n",
                "print(tokenizer.decode(generated, skip_special_tokens=True))\n"
            ],
            "metadata": {
                "id": "Rurjty1EDWP4"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "model.save_pretrained(\"./demo_qlora\")\n",
                "tokenizer.save_pretrained(\"./demo_qlora\")\n"
            ],
            "metadata": {
                "id": "YmzQW3H6DdlG"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "torch.cuda.is_available()\n"
            ],
            "metadata": {
                "id": "KnR_3VqUKBW0"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [],
            "metadata": {
                "id": "hB5v3nunKDv5"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [],
            "metadata": {
                "id": "ahswlYoQa_V2"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}
