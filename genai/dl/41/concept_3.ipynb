{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Optimizers in Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we will learn about what optimizers are, specifically focusing on two popular types: **SGD (Stochastic Gradient Descent)** and **Adam**. We'll explore how they work and see a simple code example comparing both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What are Optimizers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Definition:** Algorithms that adjust model weights to minimize loss\n",
        "\n",
        "**Analogy:** Like a smart GPS that finds the fastest route to your destination! üó∫Ô∏è\n",
        "\n",
        "- They use feedback from the loss function\n",
        "- Gradually improve predictions\n",
        "- Different strategies for different problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üö∂ Stochastic Gradient Descent (SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The Classic Approach:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Takes small steps in the direction of lower loss\n",
        "- Simple and reliable\n",
        "- Like walking downhill to find the bottom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pros:** Simple, works well\n",
        "**Cons:** Can be slow, might get stuck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Adam Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The Smart Choice:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Adapts step size automatically\n",
        "- Remembers previous updates\n",
        "- Like a smart car with adaptive cruise control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pros:** Fast, adaptive, generally better\n",
        "**Cons:** More complex, uses more memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Training a Chatbot:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- üéØ Goal: Generate human-like responses\n",
        "- üìä Loss: Measures response quality\n",
        "- ‚ö° SGD: Slow but steady improvement\n",
        "- üöÄ Adam: Faster adaptation to patterns\n",
        "\n",
        "*Adam often gets better results faster! ‚è∞*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's Compare Optimizers in Code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Time to see SGD vs Adam in action\n",
        "*We'll train a simple model and watch the difference*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Linear(10, 1)  # Simple model\n",
        "\n",
        "# SGD Optimizer\n",
        "sgd_optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Adam Optimizer\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop example\n",
        "loss_fn = nn.MSELoss()\n",
        "for epoch in range(100):\n",
        "    # Forward pass, calculate loss, update weights\n",
        "    # SGD takes consistent steps\n",
        "    # Adam adapts step size automatically\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[üöÄ Try in Colab](https://colab.research.google.com/github/Roopesht/codeexamples/blob/main/genai/python_easy/2/optimizers_comparison.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizers Made Simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **SGD:** Like walking downhill with consistent steps üö∂\n",
        "- **Adam:** Like having a smart guide that adjusts pace üß≠\n",
        "\n",
        "- Both try to minimize loss\n",
        "- Adam is usually faster and smarter\n",
        "- SGD is simpler and more predictable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Optimizer Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Watch SGD vs Adam navigate to the optimal solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Different Angle: Learning to Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **SGD Approach:** Consistent practice, same routine daily\n",
        "- **Adam Approach:** Adapts to traffic, weather, and road conditions\n",
        "\n",
        "- üöó SGD: Steady progress, might be slower\n",
        "- üèéÔ∏è Adam: Smart adjustments, usually faster learning\n",
        "\n",
        "*Both get you there, but Adam is often the smarter choice! üéØ*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time to Think!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Optimizers are like navigation systems for AI learning.**\n",
        "\n",
        "**Question:** When learning a new skill like playing guitar, would you prefer consistent daily practice (SGD) or adapting your practice based on your progress (Adam)? üé∏"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}