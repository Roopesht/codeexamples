{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activation Functions in Neural Networks\n",
        "\n",
        "Welcome! In this notebook, we will learn about activation functions, which are essential components of neural networks. They help decide whether a neuron should be activated or not based on its inputs.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What are Activation Functions?\n",
        "\n",
        "Activation functions take the input to a neuron, perform some mathematical operation, and output a value. This output determines whether the neuron activates and how much it influences the next layer.\n",
        "\n",
        "Without activation functions, neural networks would behave like simple linear models, limiting their ability to learn complex patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Activation Functions\n",
        "\n",
        "Let's look at some of the most common activation functions used in neural networks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Sigmoid Function\n",
        "\n",
        "The Sigmoid function maps any input to a value between 0 and 1. It's especially useful in the output layer for binary classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Hyperbolic Tangent (tanh)\n",
        "\n",
        "The tanh function maps inputs to values between -1 and 1. It is zero-centered, which can be beneficial for learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. ReLU (Rectified Linear Unit)\n",
        "\n",
        "ReLU outputs the input directly if it is positive; otherwise, it outputs zero. It's widely used because it helps neural networks learn faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Activation Functions\n",
        "\n",
        "Let's see what these functions look like with some example inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate example inputs\n",
        "x = np.linspace(-10, 10, 200)\n",
        "\n",
        "# Compute outputs\n",
        "y_sigmoid = sigmoid(x)\n",
        "y_tanh = tanh(x)\n",
        "y_relu = relu(x)\n",
        "\n",
        "# Plot the functions\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(x, y_sigmoid)\n",
        "plt.title('Sigmoid Function')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(x, y_tanh)\n",
        "plt.title('Tanh Function')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(x, y_relu)\n",
        "plt.title('ReLU Function')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- Activation functions decide whether a neuron should be activated.\n",
        "- Common types include Sigmoid, Tanh, and ReLU.\n",
        "- Choice of activation function impacts model performance.\n",
        "\n",
        "Now you know the basics of activation functions in neural networks!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}