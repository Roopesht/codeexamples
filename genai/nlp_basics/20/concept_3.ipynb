{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Scissors cutting a sentence into individual word pieces](images/text_being_cut_into_pieces.png)\n",
        "\n",
        "*The art of breaking text into meaningful chunks...*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Tokenization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breaking text into individual pieces called \"tokens\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- üî™ Split sentences into words\n",
        "- üìù Each word becomes a \"token\"\n",
        "- üîß Foundation for all NLP tasks\n",
        "- üéØ Makes text analysis possible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Think of it as creating a word puzzle - first, separate all the pieces!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization in Action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Input:** \"I love Python programming!\"\n",
        ">\n",
        "> **After Tokenization:**\n",
        "> `[\"I\", \"love\", \"Python\", \"programming\", \"!\"]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Another Example:** \"It's a beautiful day, isn't it?\"\n",
        ">\n",
        "> **Smart Tokenization:**\n",
        "> `[\"It\", \"'s\", \"a\", \"beautiful\", \"day\", \",\", \"isn\", \"'t\", \"it\", \"?\"]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Python Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic tokenization with Python\n",
        "text = \"Machine learning is fascinating!\"\n",
        "\n",
        "# Simple split (basic tokenization)\n",
        "tokens_basic = text.split()\n",
        "print(\"Basic:\", tokens_basic)\n",
        "# Output: ['Machine', 'learning', 'is', 'fascinating!']\n",
        "\n",
        "# Better tokenization (handling punctuation)\n",
        "import re\n",
        "tokens_better = re.findall(r'\\w+', text)\n",
        "print(\"Better:\", tokens_better)\n",
        "# Output: ['Machine', 'learning', 'is', 'fascinating']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[üöÄ Try in Colab](https://colab.research.google.com/github/Roopesht/codeexamples/blob/main/genai/python_easy/1/concept_3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization Made Simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Imagine tokenization like organizing your closet:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"font-size: 1.1em;\">\n",
        "  <p>üëï <strong>Step 1:</strong> Take everything out (your text)</p>\n",
        "  <p>üìÇ <strong>Step 2:</strong> Sort into categories (words, punctuation)</p>\n",
        "  <p>üè∑Ô∏è <strong>Step 3:</strong> Label each item (create tokens)</p>\n",
        "  <p>‚ú® <strong>Result:</strong> Everything organized and findable!</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization from a Different Angle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Think of tokenization as preparing ingredients for cooking:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- ü•ó **Raw text:** A whole salad (mixed ingredients)\n",
        "- üî™ **Tokenizer:** Your knife (cutting tool)\n",
        "- ü•ï **Tokens:** Chopped vegetables (individual pieces)\n",
        "- üçΩÔ∏è **Ready to cook:** Prepared for NLP recipes!\n",
        "\n",
        "*I hope this cooking prep analogy makes tokenization clear now!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tokenization is like creating building blocks from text...**\n",
        "\n",
        "> üí≠ How do you think tokenization might handle tricky cases like \"don't\", \"U.S.A.\", or \"üòä\"?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}