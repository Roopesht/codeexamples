{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Visual Explainer: Hands-on Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Welcome to this beginner-friendly notebook where we'll build a simple visualization of how transformers process input data.\n",
        "",
        "\n",
        "We'll cover key concepts like tokenization, embeddings, positional encoding, attention weights, and multi-head attention, all demonstrated with mock data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Tokenize the Input Sentence\n",
        "Let's start by converting an input sentence into tokens (words). In a real transformer, these would be turned into indices based on a vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(sentence):\n",
        "    '''Convert sentence to list of token indices.'''\n",
        "    tokens = sentence.lower().split()\n",
        "    # For simplicity, assign each unique word an index\n",
        "    vocab = {word: idx for idx, word in enumerate(set(tokens), start=1)}\n",
        "    token_ids = [vocab[word] for word in tokens]\n",
        "    return token_ids, vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Embeddings and Add Positional Encoding\n",
        "Let's generate random embeddings for each token and add positional information to help the model understand order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_embeddings(token_ids, embed_dim=128):\n",
        "    '''Generate random word embeddings for each token.'''\n",
        "    embeddings = np.random.rand(len(token_ids), embed_dim)\n",
        "    return embeddings\n",
        "\n",
        "def add_positional_encoding(embeddings):\n",
        "    '''Add simple positional encoding to embeddings.'''\n",
        "    seq_len, embed_dim = embeddings.shape\n",
        "    position_enc = np.zeros((seq_len, embed_dim))\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(embed_dim):\n",
        "            if i % 2 == 0:\n",
        "                position_enc[pos, i] = np.sin(pos / (10000 ** (i / embed_dim)))\n",
        "            else:\n",
        "                position_enc[pos, i] = np.cos(pos / (10000 ** ((i - 1) / embed_dim)))\n",
        "    return embeddings + position_enc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Compute Mock Attention Weights\n",
        "We'll simulate attention weights as random heatmaps to illustrate how attention might flow between words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_attention_weights(seq_len):\n",
        "    '''Generate mock attention weights for visualization.'''\n",
        "    attention = np.random.rand(seq_len, seq_len)\n",
        "    # Normalize to sum to 1 for each query\n",
        "    attention /= attention.sum(axis=1, keepdims=True)\n",
        "    return attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Visualize Attention Heatmap\n",
        "Using Seaborn, we can display the attention weights to see which words focus on each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_attention(attention, tokens):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
        "    plt.xlabel('Keys')\n",
        "    plt.ylabel('Queries')\n",
        "    plt.title('Mock Attention Weights')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Putting It All Together\n",
        "Let's define a class to encapsulate the full process and run a demo with the sentence \"The cat sat on the mat\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerVisualizer:\n",
        "    def __init__(self, embed_dim=128, num_heads=8):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        # Return token IDs and the token sequence for labeling\n",
        "        tokens = sentence.lower().split()\n",
        "        vocab = {word: idx for idx, word in enumerate(set(tokens), start=1)}\n",
        "        token_ids = [vocab[word] for word in tokens]\n",
        "        return token_ids, tokens\n",
        "\n",
        "    def embed_tokens(self, token_ids):\n",
        "        embeddings = get_embeddings(token_ids, self.embed_dim)\n",
        "        return add_positional_encoding(embeddings)\n",
        "\n",
        "    def compute_attention(self, seq_len):\n",
        "        return compute_attention_weights(seq_len)\n",
        "\n",
        "    def visualize_attention(self, attention, tokens):\n",
        "        visualize_attention(attention, tokens)\n",
        "\n",
        "    def run_full_demo(self, sentence):\n",
        "        token_ids, tokens = self.tokenize(sentence)\n",
        "        embeddings = self.embed_tokens(token_ids)\n",
        "        attention = self.compute_attention(len(tokens))\n",
        "        self.visualize_attention(attention, tokens)\n",
        "\n",
        "# Usage\n",
        "visualizer = TransformerVisualizer()\n",
        "visualizer.run_full_demo(\"The cat sat on the mat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "In this notebook, we've built a simplified visualization pipeline for transformer attention mechanisms. You learned how to tokenize input, generate embeddings, add positional encoding, and visualize attention weights.\n",
        "",
        "\n",
        "Next, you can extend this by implementing multi-head attention, layering multiple transformer blocks, or making the visualization interactive with Plotly.\n",
        "",
        "\n",
        "Keep experimenting and exploring how transformers work under the hood!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}