{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Concept 3: Hands-on Attention Demo\n",
        "\n",
        "This notebook provides a beginner-friendly introduction to how attention works in natural language processing models, with visualizations and simple demos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention Mechanism Explained\n",
        "\n",
        "Imagine a spotlight at a concert focusing on different performers, highlighting important parts of the scene. Similarly, attention in models helps focus on relevant words in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- üéØ **Selective focus:** Emphasizes important information\n",
        "- üìä **Weighted combination:** Combines info based on importance\n",
        "- üîÑ **Dynamic weights:** Changes focus depending on the context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How Attention Weights Work\n",
        "\n",
        "Let's consider the sentence: *\"The quick brown fox jumps\"*. When processing the word \"fox\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When processing \"fox\":\n",
        "\n",
        "- \"The\": 0.1 (low attention)\n",
        "- \"quick\": 0.3 (medium attention)\n",
        "- \"brown\": 0.8 (high attention)\n",
        "- \"fox\": 1.0 (self-attention)\n",
        "- \"jumps\": 0.2 (low attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention Pattern Visualization\n",
        "\n",
        "Below is a heatmap showing how attention weights are distributed among words in a sentence. Darker colors indicate stronger attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Heatmap showing attention weights between words in a sentence, with darker colors indicating stronger connections, size 800x600](images/attention_heatmap.png)\n",
        "\n",
        "**Dark areas = strong attention, Light areas = weak attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Example: Sentiment Analysis\n",
        "\n",
        "Review: \"The movie was okay but the ending was absolutely terrible\". \n",
        "\n",
        "- üéØ Processing \"terrible\" ‚Üí High attention to \"ending\".\n",
        "- üéØ Processing \"okay\" ‚Üí High attention to \"movie\".\n",
        "- ‚úÖ **Result:** The model understands mixed sentiment correctly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Demo: Attention Visualization\n",
        "\n",
        "Let's build a simple attention visualizer together!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_attention(sentence, attention_weights):\n",
        "    \"\"\"Visualize attention patterns as heatmap\"\"\"\n",
        "    words = sentence.split()\n",
        "    \n",
        "    # Create heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(attention_weights, \n",
        "                xticklabels=words, \n",
        "                yticklabels=words,\n",
        "                annot=True, \n",
        "                cmap='Blues',\n",
        "                fmt='.2f')\n",
        "    plt.title('Attention Weights Visualization')\n",
        "    plt.ylabel('Query Words')\n",
        "    plt.xlabel('Key Words')\n",
        "    plt.show()\n",
        "\n",
        "# Example attention matrix\n",
        "sentence = \"The cat sat on the mat\"\n",
        "words = sentence.split()\n",
        "n_words = len(words)\n",
        "\n",
        "# Mock attention weights (normally computed by model)\n",
        "attention_matrix = np.random.rand(n_words, n_words)\n",
        "# Make it more realistic\n",
        "np.fill_diagonal(attention_matrix, 1.0)  # High self-attention\n",
        "\n",
        "visualize_attention(sentence, attention_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can run the above code to see a heatmap that visualizes how a simple model might pay attention to different words in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concept 3 Made Simple\n",
        "\n",
        "Think of attention like reading comprehension:\n",
        "- üìñ Question: \"Who sat on the mat?\"\n",
        "- üëÅÔ∏è Your eyes automatically focus on \"cat\" and \"sat\"\n",
        "- üß† You ignore less relevant words like \"the\"\n",
        "- ‚úÖ **Attention does exactly this automatically!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Attention Demo\n",
        "\n",
        "Watch how attention weights change dynamically in this demo!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<svg id=\"attention-svg\" width=\"800\" height=\"400\" class=\"svg\"></svg>\n",
        "<p><em>Watch how attention weights change dynamically!</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concept 3 from a Different Angle\n",
        "\n",
        "Attention can be visualized as magnetic forces between words ‚Äî the stronger the force, the higher the attention.\n",
        "\n",
        "I hope attention visualization is clear now! It's like having smart spotlights illuminate important connections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n",
        "\n",
        "Attention mechanisms help models focus on the most relevant parts of the input dynamically.\n",
        "\n",
        "Challenge question: In document summarization, what words would you expect to receive the highest attention weights?"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}