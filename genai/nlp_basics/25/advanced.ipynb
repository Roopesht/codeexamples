{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Positional Encoding in Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìç Advanced: Positional Encoding Basics\n",
        "\n",
        "Transformers process all words (or tokens) in a sequence at the same time, which means they don't inherently know the order of the words. To help them understand the order, we add positional information to each word's representation.\n",
        "\n",
        "Let's explore how this works!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Position Problem\n",
        "\n",
        "**Challenge:** Transformers process all positions simultaneously ‚Äî they don‚Äôt know the order of words!\n",
        "\n",
        "- ‚ùå \"Dog bites man\" vs \"Man bites dog\" - same words, different meaning!\n",
        "- ‚ùå Without position info, both sentences look identical to the model.\n",
        "- ‚úÖ **Solution:** Add positional information to word embeddings to give each word a sense of position."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sinusoidal Positional Encoding\n",
        "\n",
        "**A clever mathematical trick:** Use sine and cosine waves to encode position information.\n",
        "\n",
        "The formulas are:\n",
        "\n",
        "PE(pos, 2i) = sin(pos / 10000^{2i/d_model})\n",
        "PE(pos, 2i+1) = cos(pos / 10000^{2i/d_model})\n",
        "\n",
        "- üåä Different frequencies for each dimension\n",
        "- üîÑ Allows the model to learn relative positions\n",
        "- üìè Works for sequences longer than seen in training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional Encoding Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def positional_encoding(seq_len, d_model):\n",
        "    \"\"\"Generate sinusoidal positional encodings\"\"\"\n",
        "    pos_enc = np.zeros((seq_len, d_model))\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))\n",
        "            if i + 1 < d_model:\n",
        "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * i / d_model)))\n",
        "    return pos_enc\n",
        "\n",
        "# Visualize positional encodings\n",
        "seq_len, d_model = 50, 128\n",
        "pos_encodings = positional_encoding(seq_len, d_model)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.pcolormesh(pos_encodings, cmap='coolwarm')\n",
        "plt.xlabel('Encoding Dimension')\n",
        "plt.ylabel('Sequence Position')\n",
        "plt.title('Positional Encoding Visualization')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üöÄ Open this in Colab\n",
        "[Open Task in Colab](https://colab.research.google.com/github/Roopesht/codeexamples/blob/main/genai/nlp_basics/25/advanced.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional Encoding Made Simple\n",
        "\n",
        "**Think of it like GPS coordinates:**\n",
        "\n",
        "- üìç Each word gets a unique \"coordinates\" based on its position\n",
        "- üó∫Ô∏è Even if words are the same, their positions make them distinguishable\n",
        "- üß≠ The model learns: \"The\" at position 1 ‚â† \"The\" at position 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why sinusoidal functions are preferred\n",
        "\n",
        "Positional encodings using sine and cosine functions are better than simple position numbers because:\n",
        "- They allow the model to learn relative positions rather than fixed absolute positions.\n",
        "- They enable generalization to longer sequences than seen during training.\n",
        "- Their continuous nature captures the notion of distance and order more effectively."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}