{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Concept 4: Pretrained Embeddings (GloVe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is GloVe?\n",
        "**GloVe:** Global Vectors for Word Representation\n",
        "**Pretrained:** Smart embeddings trained on billions of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![GloVe Embeddings](images/glove_embeddings.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Think Like Using a Language Expert\n",
        "**Your approach:** Learn word meanings from scratch\n",
        "**GloVe approach:** Use knowledge from reading entire internet!\n",
        "- Trained on Wikipedia, news, books\n",
        "- Knows relationships between millions of words\n",
        "- Ready to use immediately\n",
        "*Like hiring a linguistics professor! üë®‚Äçüéì*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World: Language Translation\n",
        "**Google Translate uses similar embeddings:**\n",
        "- Understands that \"king\" - \"man\" + \"woman\" = \"queen\"\n",
        "- Maps words across different languages\n",
        "- Captures complex word relationships automatically\n",
        "That's how it translates context, not just words! üåç"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's Explore Word Relationships!\n",
        "We'll load GloVe embeddings and discover word similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Load pretrained GloVe embeddings (simplified)\n",
        "# In real code, you'd download GloVe files\n",
        "word_vectors = {\n",
        "    'king': np.array([0.2, 0.8, 0.5]),\n",
        "    'queen': np.array([0.3, 0.7, 0.6]),\n",
        "    'man': np.array([0.1, 0.9, 0.2]),\n",
        "    'woman': np.array([0.2, 0.8, 0.3])\n",
        "}\n",
        "print(\"King embedding:\", word_vectors['king'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GloVe Made Simple\n",
        "**It's like a word wisdom database:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Each word has a \"smart\" number representation\n",
        "- Similar words are mathematically close\n",
        "- You can do word math: king - man + woman = queen!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GloVe from a Different Angle\n",
        "**Whiteboard Time! üìù**\n",
        "Imagine GloVe as a map where related words live in the same neighborhood!\n",
        "*I hope this is clear now!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Check\n",
        "**Pretrained embeddings like GloVe give us instant access to sophisticated word understanding.**\n",
        "Why would you choose pretrained embeddings over training your own from scratch?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}