{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# N-gram Language Models\n",
        "\n",
        "Welcome to this beginner-friendly introduction to N-gram models! In this notebook, we'll explore how sequences of words can help us predict what comes next in a sentence.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What are N-grams?\n",
        "\n",
        "- **N-gram**: A sequence of N consecutive words.\n",
        "- **Unigram (1-gram)**: Single word (e.g., \"cat\")\n",
        "- **Bigram (2-gram)**: Two words together (e.g., \"black cat\")\n",
        "- **Trigram (3-gram)**: Three words (e.g., \"the black cat\")\n",
        "\n",
        "**Higher N:** Gives more context but requires more data to learn effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Analogy\n",
        "\n",
        "**üéµ N-grams are like musical phrases:**\n",
        "\n",
        "- **Unigram:** Single note - \"Do\"\n",
        "- **Bigram:** Two notes - \"Do-Re\"\n",
        "- **Trigram:** Three notes - \"Do-Re-Mi\"\n",
        "- **Prediction:** After \"Do-Re\", the next note is likely \"Mi\"\n",
        "\n",
        "*Musicians recognize patterns, and N-grams do too!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## N-gram Examples in Action\n",
        "\n",
        "Text: \"The quick brown fox jumps\"\n",
        "\n",
        "- **Unigrams:** [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\"]\n",
        "- **Bigrams:** [\"The quick\", \"quick brown\", \"brown fox\", \"fox jumps\"]\n",
        "- **Trigrams:** [\"The quick brown\", \"quick brown fox\", \"brown fox jumps\"]\n",
        "\n",
        "**Prediction:** After \"fox jumps\", what's next? \"over\"?\n",
        "\n",
        "Let's see how we can build and use N-grams!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building an N-gram Model Demo\n",
        "\n",
        "We'll create a simple bigram model from some example text. This model will help us see patterns in word sequences and predict the next word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Building a simple bigram model\n",
        "text = \"I love cats. I love dogs. I love animals.\"\n",
        "words = text.replace('.', '').lower().split()\n",
        "\n",
        "# Extract bigrams\n",
        "bigrams = []\n",
        "for i in range(len(words) - 1):\n",
        "    bigram = (words[i], words[i+1])\n",
        "    bigrams.append(bigram)\n",
        "\n",
        "print(\"Bigrams found:\", bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Count occurrences for prediction\n",
        "from collections import Counter\n",
        "bigram_counts = Counter(bigrams)\n",
        "print(\"Most common:\", bigram_counts.most_common(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## N-grams Made Simple\n",
        "\n",
        "**Core Process:**\n",
        "\n",
        "- ‚úÇÔ∏è **Step 1:** Slice text into overlapping chunks\n",
        "- üìä **Step 2:** Count how often each N-gram appears\n",
        "- üîÆ **Step 3:** Use counts to predict the next word\n",
        "- üé≤ **Higher count:** More likely next word\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing N-grams\n",
        "\n",
        "**Interactive Visualization:** Building Predictions\n",
        "\n",
        "<svg id=\"ngram-svg\" width=\"800\" height=\"400\" class=\"svg\"></svg>\n",
        "\n",
        "Let's see how bigrams can help us predict \"The cat ___\"!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The N-gram Trade-off\n",
        "\n",
        "**‚öñÔ∏è Context vs Data Requirements:**\n",
        "\n",
        "- üìè **Small N (unigram):** Fast, less data needed, but no context\n",
        "- üìê **Large N (5-gram):** Rich context, but needs massive data\n",
        "- üéØ **Optimal choice:** Bigrams or trigrams are often best for many tasks\n",
        "- üöÄ **Modern Language Models:** Can handle much larger contexts efficiently\n",
        "\n",
        "*N-grams laid the foundation for today's advanced models.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "solution": false
      },
      "source": [
        "## Puzzle Time!\n",
        "\n",
        "**If you train a bigram model only on sports articles, what would happen when you ask it to complete: \"The recipe calls for...\"**\n",
        "\n",
        "*Think about how domain-specific data influences predictions.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}