{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# N-gram Predictor: Build Your Own Next-Word Prediction Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook guides you through creating a simple next-word prediction system using unigrams and bigrams.\n",
        "You will learn how to process text, build n-gram models, and predict upcoming words based on context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before building our model, we need to clean and tokenize the training text. This involves converting text to lowercase, removing punctuation, and splitting into words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n\n# Example training text\ntraining_text = \"I love cats. I love dogs. Cats love fish. Dogs love bones.\"\n\n# Function for simple text preprocessing\n\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n    # Tokenize into words\n    tokens = text.split()\n    return tokens\n\n# Preprocess the training text\ntokens = preprocess_text(training_text)\nprint(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Extract N-grams (Unigrams and Bigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will build dictionaries to count unigrams and bigrams from the tokens. These counts will help us estimate probabilities later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n\n# Initialize dictionaries for counts\nunigram_counts = defaultdict(int)\nbigram_counts = defaultdict(int)\n\n# Build unigram counts\nfor word in tokens:\n    unigram_counts[word] += 1\n\n# Build bigram counts\nfor i in range(len(tokens) - 1):\n    bigram = (tokens[i], tokens[i+1])\n    bigram_counts[bigram] += 1\n\n# Display counts\nprint(\"Unigram counts:\")\nprint(dict(unigram_counts))\nprint(\"\\nBigram counts:\")\nprint(dict(bigram_counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Calculate Probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the counts, we can estimate the probability of a word given a context (for bigrams) or just the probability of a word (for unigrams).\n",
        "For smoothing unseen n-grams, we can add a small constant (like 1) to all counts, but here we'll start with basic probability calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_unigram_probability(word):\n    total_unigrams = sum(unigram_counts.values())\n    return unigram_counts[word] / total_unigrams if word in unigram_counts else 0\n\n# Example\nprint(f\"Probability of 'cats': {calculate_unigram_probability('cats')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Predict Next Word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given a context (like a single word or phrase), we want to predict the next word. We'll look at bigram counts to find the most probable next words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_next_word(context, top_k=3):\n    # Preprocess the context\n    context = context.lower()\n    tokens_ctx = preprocess_text(context)\n    if not tokens_ctx:\n        return []\n    last_word = tokens_ctx[-1]\n    # Gather predictions\n    candidates = {}\n    total_bigrams_with_prefix = 0\n    for (w1, w2), count in bigram_counts.items():\n        if w1 == last_word:\n            candidates[w2] = count\n            total_bigrams_with_prefix += count\n    # Calculate probabilities\n    predictions = []\n    for word, count in candidates.items():\n        prob = count / total_bigrams_with_prefix if total_bigrams_with_prefix > 0 else 0\n        predictions.append((word, prob))\n    # Sort by probability\n    predictions.sort(key=lambda x: x[1], reverse=True)\n    return predictions[:top_k]\n\n# Example prediction\npredicted = predict_next_word(\"I love\")\nprint(\"Top predictions for 'I love':\")\nfor word, prob in predicted:\n    print(f\"{word} ({prob:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional: Handling Unseen Contexts with Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To improve predictions for unseen bigrams, we can apply smoothing techniques like add-one smoothing. This adjusts counts to avoid zero probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_next_word_smoothed(context, top_k=3, alpha=1):\n    # Preprocess the context\n    context = context.lower()\n    tokens_ctx = preprocess_text(context)\n    if not tokens_ctx:\n        return []\n    last_word = tokens_ctx[-1]\n    # Gather candidates with smoothing\n    candidates = {}\n    total_bigrams_with_prefix = 0\n    for (w1, w2), count in bigram_counts.items():\n        if w1 == last_word:\n            candidates[w2] = count + alpha\n    # Add fake counts for unseen words\n    all_words = set(unigram_counts.keys())\n    for word in all_words:\n        if word not in candidates:\n            candidates[word] = alpha\n    total_bigrams_with_prefix = sum(candidates.values())\n    # Calculate probabilities\n    predictions = []\n    for word, count in candidates.items():\n        prob = count / total_bigrams_with_prefix\n        predictions.append((word, prob))\n    # Sort and return top-k\n    predictions.sort(key=lambda x: x[1], reverse=True)\n    return predictions[:top_k]\n\n# Example with smoothing\npredicted_smooth = predict_next_word_smoothed(\"I love\")\nprint(\"Top smoothed predictions for 'I love':\")\nfor word, prob in predicted_smooth:\n    print(f\"{word} ({prob:.2f})\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}