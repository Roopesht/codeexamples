{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Smoothing Techniques in Language Models\n",
        "\n",
        "This notebook introduces the concept of smoothing in language models. Smoothing helps prevent the problem of zero probabilities when encountering unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéõÔ∏è Advanced: Smoothing Techniques\n",
        "\n",
        "**Handling the \"zero probability\" problem**\n",
        "\n",
        "‚ö° *Advanced technique to make Language Models more robust*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Smoothing?\n",
        "\n",
        "- üéØ **Problem:** Unseen N-grams get 0% probability\n",
        "- ‚ú® **Solution:** Redistribute probability mass\n",
        "- üõ°Ô∏è **Benefit:** Model doesn't break on new data\n",
        "- ‚öñÔ∏è **Trade-off:** Slight accuracy loss for better robustness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Smoothing Methods\n",
        "\n",
        "- ‚ûï **Add-One (Laplace):** Add 1 to all N-gram counts\n",
        "- üìä **Good-Turing:** Sophisticated probability redistribution\n",
        "- üîô **Back-off:** Fall back to smaller N-grams\n",
        "- üéØ **Interpolation:** Blend different N-gram sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Add-One Smoothing\n",
        "\n",
        "Below is an example of how add-one smoothing can be implemented in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Before smoothing: unseen bigrams get 0 probability\n",
        "bigram_counts = {\"I love\": 5, \"love cats\": 3, \"cats are\": 2}\n",
        "vocab_size = 1000\n",
        "\n",
        "# Add-one smoothing\n",
        "def smooth_probability(count, total_bigrams, vocab_size):\n",
        "    return (count + 1) / (total_bigrams + vocab_size)\n",
        "\n",
        "# Now even unseen bigrams get small probability > 0\n",
        "unseen_prob = smooth_probability(0, 10, vocab_size)\n",
        "seen_prob = smooth_probability(5, 10, vocab_size)\n",
        "\n",
        "print(f\"Unseen bigram probability: {unseen_prob:.6f}\")\n",
        "print(f\"Seen bigram probability: {seen_prob:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Result:** No more zero probabilities!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Smoothing Matters\n",
        "\n",
        "**üéØ Real-world robustness:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- üì± **Autocorrect:** Handles typos and new words\n",
        "- üåê **Search:** Works with never-seen query combinations\n",
        "- üó£Ô∏è **Speech recognition:** Manages accents and variations\n",
        "- üß† **Foundation:** Led to more sophisticated neural approaches\n",
        "\n",
        "*Modern neural networks do implicit smoothing through their architecture!*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}