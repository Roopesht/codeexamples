{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Modular Q&A Pipeline with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we will build a question-and-answer (Q&A) pipeline using modular components with LangChain. This pipeline will process a document, handle user questions, generate contextual answers, and format the responses professionally.",
        "",
        "Let's go step-by-step to understand how to assemble these components!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load the Document",
        "We'll start by loading and preparing the text document that our pipeline will analyze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example document text",
        "document_text = \"\"\"Python is a high-level programming language. It's known for its simplicity and readability. Python is widely used in data science, web development, and AI applications.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Prompt Templates",
        "Next, we'll define prompt templates for each stage: document analysis, question handling, and response formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate",
        "from langchain.chat_models import ChatOpenAI",
        "from langchain import LLMChain",
        "",
        "# Initialize the language model",
        "llm = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Template for analyzing the document",
        "doc_template = PromptTemplate(",
        "    template=\"Analyze the following document: {document}\\nProvide a summary of the main points.\",",
        "    input_variables=[\"document\"]",
        ")",
        "",
        "# Template for handling user questions",
        "qa_template = PromptTemplate(",
        "    template=\"Based on the document summary: {summary}\\nAnswer the question: {question}\",",
        "    input_variables=[\"summary\", \"question\"]",
        ")",
        "",
        "# Template for formatting the final response",
        "format_template = PromptTemplate(",
        "    template=\"Answer: {answer}\\nEnsure the response is clear and professional.\",",
        "    input_variables=[\"answer\"]",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Build Chains to Connect Components",
        "Now, we'll connect these templates with the language model using chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create chains for each step",
        "doc_chain = LLMChain(llm=llm, prompt=doc_template)",
        "qa_chain = LLMChain(llm=llm, prompt=qa_template)",
        "format_chain = LLMChain(llm=llm, prompt=format_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run the Pipeline",
        "Combine all steps to analyze a document and answer a question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_qa_pipeline(document, question):",
        "    # Step 1: Analyze the document",
        "    summary = doc_chain.run(document=document)",
        "    # Step 2: Generate an answer based on the question and summary",
        "    answer = qa_chain.run(summary=summary, question=question)",
        "    # Step 3: Format the final response",
        "    final_response = format_chain.run(answer=answer)",
        "    return final_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test the pipeline with the example provided:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_document = document_text",
        "test_question = \"What are the main uses of Python?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = run_qa_pipeline(test_document, test_question)",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congratulations! You have built a modular question-answering pipeline using LangChain components. This structure can be expanded with more advanced features and integrated into larger applications."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}