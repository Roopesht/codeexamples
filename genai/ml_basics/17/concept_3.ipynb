{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Confusion Matrix Explained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we will learn about the confusion matrix, a useful tool to understand how your classification model is performing.\n",
        "## What is a Confusion Matrix?\n",
        "- It is a visual representation of prediction vs actual results.\n",
        "- It shows exactly where your model gets confused.\n",
        "- Typically, it's a 2x2 grid for binary classification.\n",
        "- It's the foundation for calculating metrics like precision, recall, and F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Confusion Matrix Diagram](images/confusion_matrix_diagram.png)\n",
        "<table style=\"margin: 20px auto; border-collapse: collapse; font-size: 18px;\">\n",
        "  <tr>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 10px;\"></th>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 10px;\">Predicted: No</th>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 10px;\">Predicted: Yes</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 10px;\">Actual: No</th>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; background-color: #90EE90;\">True Negative (TN)</td>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; background-color: #FFB6C1;\">False Positive (FP)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th style=\"border: 1px solid #ddd; padding: 10px;\">Actual: Yes</th>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; background-color: #FFB6C1;\">False Negative (FN)</td>\n",
        "    <td style=\"border: 1px solid #ddd; padding: 10px; background-color: #90EE90;\">True Positive (TP)</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading the Matrix\n",
        "- âœ… **True Positive (TP):** Correctly predicted as positive.\n",
        "- âœ… **True Negative (TN):** Correctly predicted as negative.\n",
        "- âŒ **False Positive (FP):** Incorrectly predicted as positive (Type I error).\n",
        "- âŒ **False Negative (FN):** Incorrectly predicted as negative (Type II error).\n",
        "\n",
        "**ðŸ’¡ A perfect model** would only have TP and TN, with no FP or FN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real Example: Email Spam Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Email Spam Confusion Matrix](images/email_spam_confusion.png)\n",
        "**Scenario:** Testing spam detector on 1000 emails\n",
        "- ðŸŽ¯ **TP = 85:** Correctly identified spam emails\n",
        "- âœ… **TN = 890:** Correctly identified normal emails\n",
        "- ðŸš¨ **FP = 15:** Normal emails marked as spam\n",
        "- ðŸ˜± **FN = 10:** Spam emails that got through\n",
        "\n",
        "**Accuracy:** (85 + 890) / 1000 = 97.5%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code Example: Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Using our previous model and predictions (ensure y_test and y_pred are defined)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Visualize with heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Detailed report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaway"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">*\"Confusion matrix reveals the full story - not just accuracy!\"*\n",
        "\n",
        "### Question:\n",
        "- Looking at a confusion matrix, how would you identify if your model is biased towards positive or negative predictions?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}