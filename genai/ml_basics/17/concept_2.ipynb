{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performance Metrics in Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we will learn about important evaluation metrics used to measure the performance of classification models:\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1 Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Performance Metrics: Beyond Simple Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 🎯 **Accuracy:** Overall correctness percentage\n",
        "- 🔍 **Precision:** How many positive predictions were correct?\n",
        "- 🎣 **Recall:** How many actual positives did we catch?\n",
        "- ⚖️ **F1 Score:** Balanced combination of precision & recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Accuracy: The Starting Point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Formula:** Accuracy = (Correct Predictions) / (Total Predictions)\n",
        "\n",
        "- 📈 Easy to understand and calculate\n",
        "- ⚠️ **Problem:** Misleading with imbalanced data\n",
        "- 🏥 **Example:** 95% accuracy in rare disease detection sounds good...\n",
        "- 🚨 **Reality:** Model might just predict \"no disease\" for everyone!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔍 Precision: Quality Over Quantity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Formula:** Precision = True Positives / (True Positives + False Positives)\n",
        "\n",
        "- 🎯 **Question:** \"When I predict positive, how often am I right?\"\n",
        "- 📧 **Email Example:** Of emails marked as spam, how many are actually spam?\n",
        "- 💡 **High Precision:** Few false alarms, but might miss some\n",
        "- ⚖️ **Trade-off:** Being careful vs being thorough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎣 Recall: Catching Everything"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Formula:** Recall = True Positives / (True Positives + False Negatives)\n",
        "\n",
        "- 🔍 **Question:** \"Of all actual positives, how many did I find?\"\n",
        "- 🏥 **Medical Example:** Of all patients with disease, how many did we detect?\n",
        "- 📈 **High Recall:** Catch most cases, but more false alarms\n",
        "- ⚖️ **Trade-off:** Being thorough vs being precise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚖️ F1 Score: The Balanced Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Formula:** F1 = 2 × (Precision × Recall) / (Precision + Recall)\n",
        "\n",
        "- 🤝 **Purpose:** Harmonic mean of precision and recall\n",
        "- 📊 **Range:** 0 to 1 (higher is better)\n",
        "- ⚖️ **Best for:** Imbalanced datasets\n",
        "- 🎯 **Goal:** Balance between precision and recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 💻 Code Example: Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.data, data.target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train and predict\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall: {recall:.3f}\")\n",
        "print(f\"F1 Score: {f1:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Key Takeaway"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **\"Different metrics tell different stories - choose wisely!\"**\n",
        "\n",
        "### Reflection Question:\n",
        "- For a fraud detection system, would you prioritize precision or recall? Why?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}