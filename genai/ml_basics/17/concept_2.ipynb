{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Concept 2: Accuracy, Precision, Recall & F1 Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Core Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- üéØ **Accuracy**: Overall correctness (correct predictions / total predictions)\n",
        "- üîç **Precision**: Of positive predictions, how many were correct?\n",
        "- üìà **Recall**: Of actual positives, how many did we catch?\n",
        "- ‚öñÔ∏è **F1 Score**: Harmonic mean of precision and recall\n",
        "- üí° **Each metric tells a different story about performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visual Diagram\n",
        "",
        "![Visual diagram showing precision and recall with circles representing actual vs predicted positive cases, with overlapping areas showing true positives. Size 800x600](images/precision_recall_diagram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíª Code Example: Calculating Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Note: You need to have your data prepared in X_train, y_train, X_test, y_test\n",
        "# Example training a simple model\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall: {recall:.3f}\")\n",
        "print(f\"F1 Score: {f1:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accuracy: 0.967\n",
        "Precision: 0.969\n",
        "Recall: 0.967\n",
        "F1 Score: 0.967\n",
        "\n",
        "Model Performance Summary:\n",
        "- 96.7% of predictions were correct\n",
        "- High precision: Few false positives\n",
        "- High recall: Caught most true positives\n",
        "- Balanced F1 score indicates good overall performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Real-World Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Medical diagnosis: High recall (catch all diseases) might be more important than high precision (some false alarms OK). Email spam: High precision (don't block important emails) might be preferred!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí≠ **Question:** For a fraud detection system, would you prioritize precision or recall?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}