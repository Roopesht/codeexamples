{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interpretation of Results in Machine Learning\n",
        "\n",
        "In this notebook, we'll learn how to interpret the results of your machine learning models. Understanding the performance metrics and visualizations helps you see how well your model is doing and where it can improve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Key Evaluation Metrics\n",
        "\n",
        "When assessing your model, some common metrics are used:\n",
        "\n",
        "- **Accuracy:** How often the model predicts correctly.\n",
        "- **Precision:** How many of the predicted positives are actually positive.\n",
        "- **Recall:** How many actual positives the model correctly finds.\n",
        "- **F1-Score:** The harmonic mean of precision and recall, balancing both.\n",
        "\n",
        "üí° Remember, choose metrics based on your specific business problem!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Confusion Matrix\n",
        "\n",
        "A confusion matrix shows how many predictions your model got right and wrong for each class:\n",
        "\n",
        "![Confusion matrix example](images/confusion_matrix.png)\n",
        "\n",
        "- ‚úÖ True Positives: Correctly predicted positive cases.\n",
        "- ‚ùå False Positives: Incorrectly predicted positive cases.\n",
        "- ‚ùå False Negatives: Missed positive cases.\n",
        "- ‚úÖ True Negatives: Correctly predicted negative cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà ROC Curves and AUC\n",
        "\n",
        "ROC Curve plots the True Positive Rate against the False Positive Rate at different thresholds:\n",
        "\n",
        "![ROC curve graph](images/roc_curve.png)\n",
        "\n",
        "- **AUC (Area Under Curve):** Measures the overall ability of the model to distinguish between classes. 1.0 is perfect, 0.5 is random guessing.\n",
        "\n",
        "Higher AUC indicates a better model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Feature Importance\n",
        "\n",
        "Understanding which features influence the predictions most helps improve your model and data collection:\n",
        "\n",
        "![Feature importance bar chart](images/feature_importance.png)\n",
        "\n",
        "Use this insight to focus on the most important features!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Results Interpretation Code\n",
        "\n",
        "Let's see how to generate these evaluation reports and visuals with Python code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure you have a trained model and data split: model, X_test, y_test\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.show()\n",
        "\n",
        "# ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance (for tree-based models)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_test.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 10 Important Features:\")\n",
        "print(feature_importance.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Key Takeaway\n",
        "\n",
        "A good model not only performs well but is also interpretable and useful for decision-making!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí≠ **Think About It:** How would you explain your model's predictions to a business stakeholder?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}