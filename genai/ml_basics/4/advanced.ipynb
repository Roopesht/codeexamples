{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Data Cleaning and Feature Selection\n",
        "\n",
        "In this notebook, we will explore advanced techniques to clean data and select the best features for your machine learning models. These methods can help improve the performance of your models when used appropriately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Missing Value Strategies\n",
        "\n",
        "Handling missing data is a common task in data preprocessing. Advanced techniques go beyond simple filling with zeros or mean values. Let's look at some options:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Mean/Median:** Filling missing numerical values with the average or median.\n",
        "- **Mode:** Filling missing categorical data with the most common value.\n",
        "- **Forward/Backward Fill:** Propagating next or previous values in time series.\n",
        "- **Interpolation:** Estimating missing data points using methods like linear or spline interpolation.\n",
        "- **Predictive Imputation:** Using machine learning models to predict missing values based on other features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outlier Detection Methods\n",
        "\n",
        "Outliers are data points that differ significantly from other observations. Detecting and treating outliers can improve model stability.\n",
        "\n",
        "### Methods include:\n",
        "\n",
        "- **Z-Score:** Identifies points more than a certain number of standard deviations from the mean.\n",
        "- **IQR Method:** Uses the interquartile range to find points outside 1.5 times the IQR.\n",
        "- **Isolation Forest:** A machine learning method to detect anomalies.\n",
        "- **Domain Knowledge:** Using expertise to identify outliers relevant to the business problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Selection Techniques\n",
        "\n",
        "Selecting the most relevant features is crucial for building effective models.\n",
        "\n",
        "- **Filter Methods:** Use statistical measures like correlation or mutual information.\n",
        "- **Wrapper Methods:** Use algorithms that evaluate subsets of features, like forward or backward selection.\n",
        "- **Embedded Methods:** Feature selection integrated within model training, e.g., LASSO or Ridge regression.\n",
        "- **Recursive Feature Elimination (RFE):** Systematically removes features and keeps the most important ones.\n",
        "\n",
        "ðŸ’¡ Goal: Keep informative features and remove redundant ones!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Advanced Preprocessing Code\n",
        "\n",
        "Here's some sample Python code demonstrating these techniques using popular libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Assume df, X, y are predefined DataFrame and arrays\n",
        "\n",
        "# Advanced imputation\n",
        "# KNN imputation for numerical features\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "df_numerical = df.select_dtypes(include=[np.number])\n",
        "df_imputed = pd.DataFrame(knn_imputer.fit_transform(df_numerical), \n",
        "                         columns=df_numerical.columns)\n",
        "\n",
        "# Outlier detection using IQR\n",
        "def remove_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "# Feature selection using SelectKBest\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Recursive Feature Elimination\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rfe = RFE(estimator=rf, n_features_to_select=10)\n",
        "X_rfe = rfe.fit_transform(X, y)\n",
        "\n",
        "print(\"Advanced preprocessing complete!\")\n",
        "print(f\"Original features: {X.shape[1]}\")\n",
        "print(f\"Selected features: {X_selected.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Advanced data cleaning and feature selection techniques can greatly enhance your machine learning workflows. Remember to start simple and gradually incorporate these methods as needed for your data and problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaway\n",
        "\n",
        "Advanced preprocessing can significantly improve model performance â€” but always start with simple methods and understand your data first."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}