{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regularization Techniques in Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Regularization?\n",
        "\n",
        "Regularization is a technique used to prevent a machine learning model from overfitting the training data by adding constraints or penalties to the model's complexity.\n",
        "\n",
        "![Regularization Concept](images/regularization_concept.png)\n",
        "\n",
        "_\"Adding guardrails to prevent your model from going off track\"_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Regularization?\n",
        "\n",
        "- ðŸ›¡ï¸ Prevents overfitting by penalizing complexity\n",
        "- ðŸ“Š Improves generalization to new data\n",
        "- ðŸ” Reduces model sensitivity to noise\n",
        "- âš–ï¸ Balances bias-variance tradeoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L1 Regularization (Lasso)\n",
        "\n",
        "- ðŸŽ¯ Adds penalty based on absolute weight values\n",
        "- âœ‚ï¸ Can reduce some weights to exactly zero\n",
        "- ðŸ” Performs automatic feature selection\n",
        "- ðŸ’¡ Good when you have many irrelevant features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L2 Regularization (Ridge)\n",
        "\n",
        "- ðŸŽ¯ Adds penalty based on squared weight values\n",
        "- ðŸ”„ Shrinks weights towards zero (but not exactly zero)\n",
        "- ðŸ” Keeps all features but reduces their impact\n",
        "- ðŸ’¡ Good for handling multicollinearity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regularization in Practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load sample data\n",
        "data = load_diabetes()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# L2 Regularization (Ridge)\n",
        "ridge = Ridge(alpha=0.1)  # alpha controls regularization strength\n",
        "ridge.fit(X_scaled, y)\n",
        "\n",
        "# L1 Regularization (Lasso)\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_scaled, y)\n",
        "\n",
        "# Compare regularization effects\n",
        "print(\"Original features:\", X.shape[1])\n",
        "print(\"Ridge non-zero weights:\", np.sum(np.abs(ridge.coef_) > 0.01))\n",
        "print(\"Lasso non-zero weights:\", np.sum(np.abs(lasso.coef_) > 0.01))\n",
        "\n",
        "# Test different alpha values\n",
        "alphas = [0.01, 0.1, 1.0, 10.0]\n",
        "for alpha in alphas:\n",
        "    ridge_alpha = Ridge(alpha=alpha)\n",
        "    ridge_alpha.fit(X_scaled, y)\n",
        "    score = ridge_alpha.score(X_scaled, y)\n",
        "    print(f\"Alpha {alpha}: RÂ² = {score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roopesht/codeexamples/blob/main/genai/python_easy/3/regularization_demo.ipynb\" target=\"_blank\" class=\"colab-button\">\n",
        "  ðŸš€ Open in Colab\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regularization Key Points\n",
        "\n",
        "- **L1 (Lasso):** Feature selection + sparsity\n",
        "- **L2 (Ridge):** Smooth weight reduction\n",
        "\n",
        "_ðŸ’­ Question: When would you choose L1 over L2 regularization?_"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}